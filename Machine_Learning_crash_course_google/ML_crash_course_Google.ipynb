{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Crash Course\n",
    "resource: https://developers.google.com/machine-learning/crash-course/ml-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing\n",
    "\n",
    "__What is (supervised) machine learning? Concisely put, it is the following:\n",
    "\n",
    "- ML systems learn how to combine input to produce useful predictions on never-before-seen data.\n",
    "\n",
    "Let's explore fundamental machine learning terminology.\n",
    "\n",
    "__Labels__\n",
    "A label is the thing we're predicting—the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.\n",
    "\n",
    "__Features__\n",
    "A feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features, specified as:\n",
    "x1,x2......xn\n",
    "\n",
    "In the spam detector example, the features could include the following:\n",
    "\n",
    "- words in the email text\n",
    "- sender's address\n",
    "- time of day the email was sent\n",
    "- email contains the phrase \"one weird trick.\"\n",
    "\n",
    "An example is a particular instance of data, x. (We put x in boldface to indicate that it is a vector.) We break examples into two categories:\n",
    "\n",
    "- labeled examples\n",
    "- unlabeled examples\n",
    "\n",
    "A labeled example includes both feature(s) and the label. That is:\n",
    "\n",
    "labeled examples: {features, label}: (x, y)\n",
    "\n",
    "_Use labeled examples to train the model. In our spam detector example, the labeled examples would be individual emails that users have explicitly marked as \"spam\" or \"not spam.\"_\n",
    "\n",
    "An unlabeled example contains features but not the label. That is:\n",
    "\n",
    "unlabeled examples: {features, ?}: (x, ?)\n",
    "\n",
    "__Models__\n",
    "\n",
    "A model defines the relationship between features and label. For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life:\n",
    "\n",
    "Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.\n",
    "\n",
    "Inference(testing) means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference(testing), you can predict medianHouseValue for new unlabeled examples.\n",
    "\n",
    "__Regression vs. classification__\n",
    "A regression model predicts continuous values. For example, regression models make predictions that answer questions like the following:\n",
    "\n",
    "- What is the value of a house in California?\n",
    "\n",
    "- What is the probability that a user will click on this ad?\n",
    "\n",
    "A classification model predicts discrete values. For example, classification models make predictions that answer questions like the following:\n",
    "\n",
    "- Is a given email message spam or not spam?\n",
    "\n",
    "- Is this an image of a dog, a cat, or a hamster?\n",
    "\n",
    "### Check your Understanding:\n",
    "\n",
    "01. Suppose you want to develop a supervised machine learning model to predict whether a given email is \"spam\" or \"not spam.\" Which of the following statements are true?\n",
    "\n",
    " - Emails not marked as \"spam\" or \"not spam\" are unlabeled examples.\n",
    " - The labels applied to some examples might be unreliable.\n",
    "\n",
    "02. Suppose an online shoe store wants to create a supervised ML model that will provide personalized shoe recommendations to users. That is, the model will recommend certain pairs of shoes to Marty and different pairs of shoes to Janet. The system will use past user behavior data to generate training data. Which of the following statements are true?\n",
    "\n",
    " - \"The user clicked on the shoe's description\" is a useful label.\n",
    " - \"Shoe size\" is a useful feature.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending into ML\n",
    "\n",
    "__Linear regression__ is a method for finding the straight line or hyperplane that best fits a set of points. This module explores linear regression intuitively before laying the groundwork for a machine learning approach to linear regression.\n",
    "\n",
    "Linear Regression is the line of best fit among the data points present the line doesn't pass through every dot, but the line does clearly show the relationship between x and y. Using the equation for a line, you could write down this relationship as follows:\n",
    "\n",
    "where: \n",
    "\n",
    "y = mx+c\n",
    "\n",
    "- y is the the value we're trying to predict.\n",
    "- m is the slope of the line.\n",
    "- x is the  value of our input feature.\n",
    "- c is the y-intercept.\n",
    "By convention in machine learning, you'll write the equation for a model slightly differently:\n",
    "\n",
    "where:\n",
    "\n",
    "y' = w1x1 + c\n",
    "\n",
    "- y' is the predicted label (a desired output).\n",
    "- c is the bias (the y-intercept), sometimes referred to as w0.\n",
    "- w1 is the weight of feature 1. Weight is the same concept as the \"slope\"  in the traditional equation of a line.\n",
    "- x1 is a feature (a known input).\n",
    "To infer (predict) the a new y' value , just substitute the  value into this model.\n",
    "\n",
    "Although this model uses only one feature, a more sophisticated model might rely on multiple features, each having a separate weight (w1,w2 , etc.). For example, a model that relies on three features might look as follows:\n",
    "\n",
    "y' = w1x1+w2x2+c\n",
    "\n",
    "### Descending into ML: Training and Loss\n",
    "\n",
    "__Training__ a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called __empirical risk minimization.__\n",
    "\n",
    "__Loss is the penalty for a bad prediction.__ That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.\n",
    "\n",
    "__Squared loss: a popular loss function__\n",
    "The linear regression models we'll examine here use a loss function called squared loss (also known as L2 loss). The squared loss for a single example is as follows:\n",
    "\n",
    " = the square of the difference between the label and the prediction\n",
    " \n",
    " = (observation - prediction(x))2\n",
    " \n",
    " = (y - y')2\n",
    " \n",
    "__Mean square error (MSE)__ is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples.\n",
    "\n",
    "\n",
    "### Check your understanding\n",
    "\n",
    "01. Which of the two data sets shown in the preceding plots has the higher Mean Squared Error (MSE)?\n",
    "    - The dataset on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "To train a model, we need a good way to reduce the model’s loss. An iterative approach is one widely used method for reducing loss, and is as easy and efficient as walking down a hill.\n",
    "\n",
    "### Reducing Loss: An Iterative Approach\n",
    "\n",
    "The previous module introduced the concept of loss. Here, in this module, you'll learn how a machine learning model iteratively reduces loss.\n",
    "\n",
    "Iterative learning might remind you of the \"Hot and Cold\" kid's game for finding a hidden object like a thimble. In this game, the \"hidden object\" is the best possible model. You'll start with a wild guess (\"The value of w1 is 0.\") and wait for the system to tell you what the loss is. Then, you'll try another guess (\"The value of w1 is 0.5.\") and see what the loss is. Aah, you're getting warmer. Actually, if you play this game right, you'll usually be getting warmer. The real trick to the game is trying to find the best possible model as efficiently as possible.\n",
    "\n",
    "We'll use this same iterative approach throughout the Machine Learning Crash Course, detailing various complications, particularly within that stormy cloud labeled \"Model (Prediction Function).\" Iterative strategies are prevalent in machine learning, primarily because they scale so well to large data sets.\n",
    "\n",
    "The \"model\" takes one or more features as input and returns one prediction (y') as output. To simplify, consider a model that takes one feature and returns one prediction:\n",
    "                                \n",
    "                                y' = b+w1x1\n",
    "\n",
    "What initial values should we set for  and ? For linear regression problems, it turns out that the starting values aren't important. We could pick random values, but we'll just take the following trivial values instead:\n",
    "\n",
    "b = 0\n",
    "w1 = 0\n",
    "Suppose that the first feature value is 10. Plugging that feature value into the prediction function yields:\n",
    "\n",
    "                                y' = 0+0.10\n",
    "\n",
    "The \"Compute Loss\" part of the diagram is the loss function that the model will use. Suppose we use the squared loss function. The loss function takes in two input values:\n",
    "                            Loss = (y-y')^2\n",
    "\n",
    "y': The model's prediction for features x\n",
    "\n",
    "y: The correct label corresponding to features x.\n",
    "\n",
    "At last, we've reached the \"Compute parameter updates\" part of the diagram. It is here that the machine learning system examines the value of the loss function and generates new values for  and . For now, just assume that this mysterious box devises new values and then the machine learning system re-evaluates all those features against all those labels, yielding a new value for the loss function, which yields new parameter values. And the learning continues iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has converged.\n",
    "\n",
    "___A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss.___\n",
    "\n",
    "### Reducing Loss: Gradient Descent\n",
    "\n",
    "The iterative approach contained a green hand-wavy box entitled \"Compute parameter updates.\" We'll now replace that algorithmic fairy dust with something more substantial.\n",
    "\n",
    "Suppose we had the time and the computing resources to calculate the loss for all possible values of w1. For the kind of regression problems we've been examining, the resulting plot of loss vs.w1 will always be convex. In other words, the plot will always be bowl-shaped.\n",
    "\n",
    "Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.\n",
    "\n",
    "Calculating the loss function for every conceivable value of  over the entire data set would be an inefficient way of finding the convergence point. Let's examine a better mechanism—very popular in machine learning—called __gradient descent.__\n",
    "\n",
    "The first stage in gradient descent is to pick a starting value (a starting point) for w1. The starting point doesn't matter much; therefore, many algorithms simply set w1 to 0 or pick a random value. \n",
    "\n",
    "The gradient descent algorithm then calculates the gradient of the loss curve at the starting point. The gradient of the loss is equal to the derivative (slope) of the curve, and tells you which way is \"warmer\" or \"colder.\" When there are multiple weights, the gradient is a vector of partial derivatives with respect to the weights.\n",
    "\n",
    "Note that a gradient is a vector, so it has both of the following characteristics:\n",
    "\n",
    "- a direction\n",
    "- a magnitude\n",
    "\n",
    "The gradient always points in the direction of steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.\n",
    "\n",
    "The gradient descent then repeats this process, edging ever closer to the minimum.\n",
    "\n",
    "___When performing gradient descent, we generalize the above process to tune all the model parameters simultaneously. For example, to find the optimal values of both w1 and the bias b , we calculate the gradients with respect to both w1 and b . Next, we modify the values of w1 and b based on their respective gradients. Then we repeat these steps until we reach minimum loss.___\n",
    "\n",
    "### Reducing Loss: Learning Rate\n",
    "\n",
    "As noted, the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the __learning rate__ (also sometimes called step size) to determine the next point. For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.\n",
    "\n",
    "__Hyperparameters__ are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. If you pick a learning rate that is too small, learning will take too long\n",
    "\n",
    "Conversely, if you specify a learning rate that is too large, the next point will perpetually bounce haphazardly across the bottom of the well like a quantum mechanics experiment gone horribly wrong\n",
    "\n",
    "There's a Goldilocks learning rate for every regression problem. The Goldilocks value is related to how flat the loss function is. If you know the gradient of the loss function is small then you can safely try a larger learning rate, which compensates for the small gradient and results in a larger step size.\n",
    "\n",
    "### Reducing Loss: Optimizing Learning Rate\n",
    "\n",
    "___In practice, finding a \"perfect\" (or near-perfect) learning rate is not essential for successful model training. The goal is to find a learning rate large enough that gradient descent converges efficiently, but not so large that it never converges.___\n",
    "\n",
    "### Reducing Loss: Stochastic Gradient Descent\n",
    "\n",
    "In gradient descent, a __batch__ is the total number of examples you use to calculate the gradient in a single iteration. So far, we've assumed that the batch has been the entire data set. When working at Google scale, data sets often contain billions or even hundreds of billions of examples. Furthermore, Google data sets often contain huge numbers of features. Consequently, a batch can be enormous. A very large batch may cause even a single iteration to take a very long time to compute.\n",
    "\n",
    "A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches.\n",
    "\n",
    "What if we could get the right gradient on average for much less computation? By choosing examples at random from our data set, we could estimate (albeit, noisily) a big average from a much smaller one. Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "\n",
    "__Mini-batch stochastic gradient descent__ (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.\n",
    "\n",
    "To simplify the explanation, we focused on gradient descent for a single feature. Rest assured that gradient descent also works on feature sets that contain multiple features.\n",
    "\n",
    "### Reducing Loss: Check Your Understanding\n",
    "\n",
    "01. When performing gradient descent on a large data set, which of the following batch sizes will likely be more efficient?\n",
    "    - A small batch or even a batch of one example (SGD).\n",
    "     Amazingly enough, performing gradient descent on a small batch or even a batch of one example is usually more efficient than the full batch. After all, finding the gradient of one example is far cheaper than finding the gradient of millions of examples. To ensure a good representative sample, the algorithm scoops up another random small batch (or batch of one) on every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow\n",
    "\n",
    "TensorFlow is an end-to-end open source platform for machine learning. TensorFlow is a rich system for managing all aspects of a machine learning system; however, this class focuses on using a particular TensorFlow API to develop and train machine learning models.\n",
    "\n",
    "Tensorflow documentation: https://www.tensorflow.org/\n",
    "\n",
    "TensorFlow APIs are arranged hierarchically, with the high-level APIs built on the low-level APIs. Machine learning researchers use the low-level APIs to create and explore new machine learning algorithms. In this class, you will use a high-level API named tf.keras to define and train machine learning models and to make predictions. tf.keras is the TensorFlow variant of the open-source Keras API.\n",
    "\n",
    "### First Steps with TensorFlow: Programming Exercises\n",
    "\n",
    "As you progress through Machine Learning Crash Course, you'll put machine learning concepts into practice by coding models in tf.keras. You'll use Colab as a programming environment. Colab is Google's version of Jupyter Notebook. Like Jupyter Notebook, Colab provides an interactive Python programming environment that combines text, code, graphics, and program output.\n",
    "\n",
    "#### NumPy and pandas\n",
    "Using tf.keras requires at least a little understanding of the following two open-source Python libraries:\n",
    "\n",
    "- __NumPy__, which simplifies representing arrays and performing linear algebra operations.\n",
    "- __pandas__, which provides an easy way to represent datasets in memory.\n",
    "\n",
    "\n",
    "Machine Learning Glossary: https://developers.google.com/machine-learning/glossary\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "__Most machine learning problems require a lot of hyperparameter tuning. Unfortunately, we can't provide concrete tuning rules for every model. Lowering the learning rate can help one model converge efficiently but make another model converge much too slowly. You must experiment to find the best set of hyperparameters for your dataset. That said, here are a few rules of thumb:__\n",
    "\n",
    "- Training loss should steadily decrease, steeply at first, and then more slowly until the slope of the curve reaches or approaches zero.\n",
    "- If the training loss does not converge, train for more epochs.\n",
    "- If the training loss decreases too slowly, increase the learning rate. Note that setting the training loss too high may also prevent training loss from converging.\n",
    "- If the training loss varies wildly (that is, the training loss jumps around), decrease the learning rate.\n",
    "- Lowering the learning rate while increasing the number of epochs or the batch size is often a good combination.\n",
    "- Setting the batch size to a very small batch number can also cause instability. First, try large batch size values. Then, decrease the batch size until you see degradation.\n",
    "- For real-world datasets consisting of a very large number of examples, the entire dataset might not fit into memory. In such cases, you'll need to reduce the batch size to enable a batch to fit into memory.\n",
    "- Remember: the ideal combination of hyperparameters is data dependent, so you must always experiment and verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalization\n",
    "\n",
    "Generalization refers to your model's ability to adapt properly to new, previously unseen data, drawn from the same distribution as the one used to create the model.\n",
    "\n",
    "### Generalization: Peril of Overfitting\n",
    "\n",
    "This module focuses on generalization. In order to develop some intuition about this concept, you're going to look at three figures. Assume that each dot in these figures represents a tree's position in a forest. The two colors have the following meanings:\n",
    "\n",
    "- The blue dots represent sick trees.\n",
    "- The orange dots represent healthy trees.\n",
    "\n",
    "An overfit model gets a low loss during training but does a poor job predicting new data. If a model fits the current sample well, how can we trust that it will make good predictions on new data? Overfitting is caused by making a model more complex than necessary. The fundamental tension of machine learning is between fitting our data well, but also fitting the data as simply as possible.\n",
    "\n",
    "Machine learning's goal is to predict well on new data drawn from a (hidden) true probability distribution. Unfortunately, the model can't see the whole truth; the model can only sample from a training data set. If a model fits the current examples well, how can you trust the model will also make good predictions on never-before-seen examples?\n",
    "\n",
    "William of Ockham, a 14th century friar and philosopher, loved simplicity. He believed that scientists should prefer simpler formulas or theories over more complex ones. To put Ockham's razor in machine learning terms:\n",
    "`The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample.`\n",
    "\n",
    "In modern times, we've formalized Ockham's razor into the fields of __statistical learning theory__ and __computational learning theory__. These fields have developed __generalization__ bounds--a statistical description of a model's ability to generalize to new data based on factors such as:\n",
    "\n",
    "- the complexity of the model\n",
    "- the model's performance on training data\n",
    "\n",
    "While the theoretical analysis provides formal guarantees under idealized assumptions, they can be difficult to apply in practice. Machine Learning Crash Course focuses instead on empirical evaluation to judge a model's ability to generalize to new data.\n",
    "\n",
    "A machine learning model aims to make good predictions on new, previously unseen data. But if you are building a model from your data set, how would you get the previously unseen data? Well, one way is to divide your data set into two subsets:\n",
    "\n",
    "- __training set__—a subset to train a model.\n",
    "- __test set__—a subset to test the model.\n",
    "\n",
    "Good performance on the test set is a useful indicator of good performance on the new data in general, assuming that:\n",
    "\n",
    "- The test set is large enough.\n",
    "- You don't cheat by using the same test set over and over.\n",
    "\n",
    "### The ML fine print\n",
    "The following three basic assumptions guide generalization:\n",
    "\n",
    "- We draw examples independently and identically (i.i.d) at random from the distribution. In other words, examples don't influence each other. (An alternate explanation: i.i.d. is a way of referring to the randomness of variables.)\n",
    "- The distribution is stationary; that is the distribution doesn't change within the data set.\n",
    "- We draw examples from partitions from the same distribution.\n",
    "\n",
    "In practice, we sometimes violate these assumptions. For example:\n",
    "\n",
    "- Consider a model that chooses ads to display. The i.i.d. assumption would be violated if the model bases its choice of ads, in part, on what ads the user has previously seen.\n",
    "- Consider a data set that contains retail sales information for a year. User's purchases change seasonally, which would violate stationarity.\n",
    "- When we know that any of the preceding three basic assumptions are violated, we must pay careful attention to metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Test Sets\n",
    "\n",
    "A test set is a data set used to evaluate the model developed from a training set.\n",
    "\n",
    "### Training and Test Sets: Splitting Data\n",
    "\n",
    "The previous module introduced the idea of dividing your data set into two subsets:\n",
    "\n",
    "- __training set__—a subset to train a model.\n",
    "- __test set__—a subset to test the trained model.\n",
    "\n",
    "Make sure that your test set meets the following two conditions:\n",
    "\n",
    "- Is large enough to yield statistically meaningful results.\n",
    "- Is representative of the data set as a whole. In other words, don't pick a test set with different characteristics than the training set.\n",
    "Assuming that your test set meets the preceding two conditions, your goal is to create a model that generalizes well to new data. Our test set serves as a proxy for new data. For example, consider the following figure. Notice that the model learned for the training data is very simple. This model doesn't do a perfect job—a few predictions are wrong. However, this model does about as well on the test data as it does on the training data. In other words, this simple model does not overfit the training data.\n",
    "\n",
    "__Never train on test data.__ If you are seeing surprisingly good results on your evaluation metrics, it might be a sign that you are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set.\n",
    "\n",
    "For example, consider a model that predicts whether an email is spam, using the subject line, email body, and sender's email address as features. We apportion the data into training and test sets, with an 80-20 split. After training, the model achieves 99% precision on both the training set and the test set. We'd expect a lower precision on the test set, so we take another look at the data and discover that many of the examples in the test set are duplicates of examples in the training set (we neglected to scrub duplicate entries for the same spam email from our input database before splitting the data). We've inadvertently trained on some of our test data, and as a result, we're no longer accurately measuring how well our model generalizes to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set: Check Your Intuition\n",
    "Before beginning this module, consider whether there are any pitfalls in using the training process outlined in Training and Test Sets.\n",
    "\n",
    "1. We looked at a process of using a test set and a training set to drive iterations of model development. On each iteration, we'd train on the training data and evaluate on the test data, using the evaluation results on test data to guide choices of and changes to various model hyperparameters like learning rate and features. Is there anything wrong with this approach? (Pick only one answer.)\n",
    "\n",
    "- Ans. Doing many rounds of this procedure might cause us to implicitly fit to the peculiarities of our specific test set.\n",
    "\n",
    "### Validation Set\n",
    "Partitioning a data set into a training set and test set lets you judge whether a given model will generalize well to new data. However, using only two partitions may be insufficient when doing many rounds of hyperparameter tuning.\n",
    "\n",
    "### Validation Set: Another Partition\n",
    "The previous module introduced partitioning a data set into a training set and a test set. This partitioning enabled you to train on one set of examples and then to test the model against a different set of examples\n",
    "\n",
    "\"Tweak model\" means adjusting anything about the model you can dream up—from changing the learning rate, to adding or removing features, to designing a completely new model from scratch. At the end of this workflow, you pick the model that does best on the test set.\n",
    "\n",
    "Dividing the data set into two sets is a good idea, but not a panacea. You can greatly reduce your chances of overfitting by partitioning the data set into the three subsets.\n",
    "\n",
    "Use the __validation set__ to evaluate results from the training set. Then, use the test set to double-check your evaluation after the model has \"passed\" the validation set.\n",
    "\n",
    "In this improved workflow:\n",
    "\n",
    "1. Pick the model that does best on the validation set.\n",
    "2. Double-check that model against the test set.\n",
    "This is a better workflow because it creates fewer exposures to the test set.\n",
    "\n",
    "_Test sets and validation sets \"wear out\" with repeated use. That is, the more you use the same data to make decisions about hyperparameter settings or other model improvements, the less confidence you'll have that these results actually generalize to new, unseen data._\n",
    "\n",
    "_If possible, it's a good idea to collect more data to \"refresh\" the test set and validation set. Starting anew is a great reset._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation\n",
    "A machine learning model can't directly see, hear, or sense input examples. Instead, you must create a representation of the data to provide the model with a useful vantage point into the data's key qualities. That is, in order to train a model, you must choose the set of features that best represent the data.\n",
    "\n",
    "### Representation: Feature Engineering\n",
    "In traditional programming, the focus is on code. In machine learning projects, the focus shifts to representation. That is, one way developers hone a model is by adding and improving its features.\n",
    "\n",
    "__Mapping Raw Data to Features__\n",
    "\n",
    "Feature engineering means transforming raw data into a feature vector. Expect to spend significant time doing feature engineering.\n",
    "\n",
    "Many machine learning models must represent the features as real-numbered vectors since the feature values must be multiplied by the model weights.\n",
    "\n",
    "__Mapping numeric values__\n",
    "Integer and floating-point data don't need a special encoding because they can be multiplied by a numeric weight.\n",
    "\n",
    "__Mapping categorical values__\n",
    "Categorical features have a discrete set of possible values. For example, there might be a feature called street_name with options that include:\n",
    "\n",
    "`{'Charleston Road', 'North Shoreline Boulevard', 'Shorebird Way', 'Rengstorff Avenue'}`\n",
    "\n",
    "Since models cannot multiply strings by the learned weights, we use feature engineering to convert strings to numeric values.\n",
    "We can accomplish this by defining a mapping from the feature values, which we'll refer to as the __vocabulary__ of possible values, to integers. Since not every street in the world will appear in our dataset, we can group all other streets into a catch-all __\"other\"__ category, known as an __OOV (out-of-vocabulary) bucket__\n",
    "\n",
    "Using this approach, here's how we can map our street names to numbers:\n",
    "\n",
    "- map Charleston Road to 0\n",
    "- map North Shoreline Boulevard to 1\n",
    "- map Shorebird Way to 2\n",
    "- map Rengstorff Avenue to 3\n",
    "- map everything else (OOV) to 4\n",
    "\n",
    "However, if we incorporate these index numbers directly into our model, it will impose some constraints that might be problematic:\n",
    "- We'll be learning a single weight that applies to all streets. For example, if we learn a weight of 6 for street_name, then we will multiply it by 0 for Charleston Road, by 1 for North Shoreline Boulevard, 2 for Shorebird Way and so on. Consider a model that predicts house prices using `street_name` as a feature. It is unlikely that there is a linear adjustment of price based on the street name, and furthermore this would assume you have ordered the streets based on their average house price. Our model needs the flexibility of learning different weights for each street that will be added to the price estimated using the other features.\n",
    "\n",
    "- We aren't accounting for cases where street_name may take multiple values. For example, many houses are located at the corner of two streets, and there's no way to encode that information in the street_name value if it contains a single index.\n",
    "\n",
    "To remove both these constraints, we can instead create a binary vector for each categorical feature in our model that represents values as follows:\n",
    "\n",
    "- For values that apply to the example, set corresponding vector elements to 1.\n",
    "- Set all other elements to 0.\n",
    "\n",
    "The length of this vector is equal to the number of elements in the vocabulary. This representation is called a __one-hot encoding__ when a single value is 1, and a __multi-hot encoding__ when multiple values are 1\n",
    "\n",
    "This approach effectively creates a Boolean variable for every feature value (e.g., street name). Here, if a house is on Shorebird Way then the binary value is 1 only for Shorebird Way. Thus, the model uses only the weight for Shorebird Way.\n",
    "\n",
    "Similarly, if a house is at the corner of two streets, then two binary values are set to 1, and the model uses both their respective weights.\n",
    "\n",
    "___One-hot encoding extends to numeric data that you do not want to directly multiply by a weight, such as a postal code.___\n",
    "\n",
    "__Sparse Representation__\n",
    "Suppose that you had 1,000,000 different street names in your data set that you wanted to include as values for street_name. Explicitly creating a binary vector of 1,000,000 elements where only 1 or 2 elements are true is a very inefficient representation in terms of both storage and computation time when processing these vectors. In this situation, a common approach is to use a sparse representation in which only nonzero values are stored. In sparse representations, an independent model weight is still learned for each feature value, as described above.\n",
    "\n",
    "### Representation: Qualities of Good Features\n",
    "\n",
    "We've explored ways to map raw data into suitable feature vectors, but that's only part of the work. We must now explore what kinds of values actually make good features within those feature vectors.\n",
    "\n",
    "__Avoid rarely used discrete feature values__\n",
    "Good feature values should appear more than 5 or so times in a data set. Doing so enables a model to learn how this feature value relates to the label. That is, having many examples with the same discrete value gives the model a chance to see the feature in different settings, and in turn, determine when it's a good predictor for the label. For example, a `house_type` feature would likely contain many examples in which its value was `victorian`:\n",
    "\n",
    "`house_type: victorian`\n",
    "\n",
    "Conversely, if a feature's value appears only once or very rarely, the model can't make predictions based on that feature. For example, `unique_house_id` is a bad feature because each value would be used only once, so the model couldn't learn anything from it:\n",
    "\n",
    "`unique_house_id: 8SK982ZZ1242Z`\n",
    "\n",
    "__Prefer clear and obvious meanings__\n",
    "Each feature should have a clear and obvious meaning to anyone on the project. For example, the following good feature is clearly named and the value makes sense with respect to the name:\n",
    "\n",
    "`house_age_years: 27`\n",
    "\n",
    "Conversely, the meaning of the following feature value is pretty much indecipherable to anyone but the engineer who created it:\n",
    "\n",
    "`house_age: 851472000`\n",
    "\n",
    "In some cases, noisy data (rather than bad engineering choices) causes unclear values. For example, the following user_age_years came from a source that didn't check for appropriate values:\n",
    "\n",
    "`user_age_years: 277`\n",
    "\n",
    "__Don't mix \"magic\" values with actual data__\n",
    "Good floating-point features don't contain peculiar out-of-range discontinuities or \"magic\" values. For example, suppose a feature holds a floating-point value between 0 and 1. So, values like the following are fine:\n",
    "`\n",
    "quality_rating: 0.82\n",
    "quality_rating: 0.37\n",
    "`\n",
    "\n",
    "However, if a user didn't enter a `quality_rating`, perhaps the data set represented its absence with a magic value like the following:\n",
    "\n",
    "`quality_rating: -1`\n",
    "\n",
    "To explicitly mark magic values, create a Boolean feature that indicates whether or not a `quality_rating` was supplied. Give this Boolean feature a name like `is_quality_rating_defined`.\n",
    "\n",
    "In the original feature, replace the magic values as follows:\n",
    "\n",
    "- For variables that take a finite set of values (discrete variables), add a new value to the set and use it to signify that the feature value is missing.\n",
    "- For continuous variables, ensure missing values do not affect the model by using the mean value of the feature's data.\n",
    "\n",
    "__Account for upstream instability__\n",
    "The definition of a feature shouldn't change over time. For example, the following value is useful because the city name probably won't change. (Note that we'll still need to convert a string like \"br/sao_paulo\" to a one-hot vector.)\n",
    "\n",
    "`city_id: \"br/sao_paulo\"`\n",
    "\n",
    "But gathering a value inferred by another model carries additional costs. Perhaps the value \"219\" currently represents Sao Paulo, but that representation could easily change on a future run of the other model:\n",
    "\n",
    "`inferred_city_cluster: \"219\"`\n",
    "\n",
    "### Representation: Cleaning Data\n",
    "\n",
    "Apple trees produce some mixture of great fruit and wormy messes. Yet the apples in high-end grocery stores display 100% perfect fruit. Between orchard and grocery, someone spends significant time removing the bad apples or throwing a little wax on the salvageable ones. As an ML engineer, you'll spend enormous amounts of your time tossing out bad examples and cleaning up the salvageable ones. Even a few \"bad apples\" can spoil a large data set.\n",
    "\n",
    "__Scaling feature values__\n",
    "Scaling means converting floating-point feature values from their natural range (for example, 100 to 900) into a standard range (for example, 0 to 1 or -1 to +1). If a feature set consists of only a single feature, then scaling provides little to no practical benefit. If, however, a feature set consists of multiple features, then feature scaling provides the following benefits:\n",
    "\n",
    "- Helps gradient descent converge more quickly.\n",
    "- Helps avoid the \"NaN trap,\" in which one number in the model becomes a NaN (e.g., when a value exceeds the floating-point precision limit during training), and—due to math operations—every other number in the model also eventually becomes a NaN.\n",
    "- Helps the model learn appropriate weights for each feature. Without feature scaling, the model will pay too much attention to the features having a wider range.\n",
    "\n",
    "__You don't have to give every floating-point feature exactly the same scale. Nothing terrible will happen if Feature A is scaled from -1 to +1 while Feature B is scaled from -3 to +3. However, your model will react poorly if Feature B is scaled from 5000 to 100000.__\n",
    "\n",
    "__Handling extreme outliers__\n",
    "\n",
    "How could we minimize the influence of those extreme outliers? Well, one way would be to take the log of every value:\n",
    "\n",
    "Log scaling does a slightly better job, but there's still a significant tail of outlier values.\n",
    "\n",
    "__Clipping__ the feature value at 4.0 doesn't mean that we ignore all values greater than 4.0. Rather, it means that all values that were greater than 4.0 now become 4.0. This explains the funny hill at 4.0. Despite that hill, the scaled feature set is now more useful than the original data.\n",
    "\n",
    "__Binning__\n",
    "The following plot shows the relative prevalence of houses at different latitudes in California. Notice the clustering—Los Angeles is about at latitude 34 and San Francisco is roughly at latitude 38.\n",
    "\n",
    "In the data set, `latitude` is a floating-point value. However, it doesn't make sense to represent `latitude` as a floating-point feature in our model. That's because no linear relationship exists between latitude and housing values. For example, houses in latitude 35 are not  more expensive (or less expensive) than houses at latitude 34. And yet, individual latitudes probably are a pretty good predictor of house values.\n",
    "\n",
    "To make latitude a helpful predictor, let's divide latitudes into \"bins\"\n",
    "\n",
    "Instead of having one floating-point feature, we now have 11 distinct boolean features `(LatitudeBin1, LatitudeBin2, ..., LatitudeBin11)`. Having 11 separate features is somewhat inelegant, so let's unite them into a single 11-element vector. Doing so will enable us to represent latitude 37.4 as follows:\n",
    "\n",
    "`[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`\n",
    "\n",
    "Another approach is to bin by quantile, which ensures that the number of examples in each bucket is equal. Binning by quantile completely removes the need to worry about outliers.\n",
    "\n",
    "__Scrubbing__\n",
    "Until now, we've assumed that all the data used for training and testing was trustworthy. In real-life, many examples in data sets are unreliable due to one or more of the following:\n",
    "\n",
    "- Omitted values. For instance, a person forgot to enter a value for a house's age.\n",
    "- Duplicate examples. For example, a server mistakenly uploaded the same logs twice.\n",
    "- Bad labels. For instance, a person mislabeled a picture of an oak tree as a maple.\n",
    "- Bad feature values. For example, someone typed in an extra digit, or a thermometer was left out in the sun.\n",
    "Once detected, you typically \"fix\" bad examples by removing them from the data set. To detect omitted values or duplicated examples, you can write a simple program. Detecting bad feature values or labels can be far trickier.\n",
    "\n",
    "In addition to detecting bad individual examples, you must also detect bad data in the aggregate. Histograms are a great mechanism for visualizing your data in the aggregate. In addition, getting statistics like the following can help:\n",
    "\n",
    "- Maximum and minimum\n",
    "- Mean and median\n",
    "- Standard deviation\n",
    "\n",
    "Consider generating lists of the most common values for discrete features. For example, do the number of examples with `country:uk `match the number you expect. Should `language:jp` really be the most common language in your data set?\n",
    "\n",
    "__Know your data__\n",
    "Follow these rules:\n",
    "\n",
    "- Keep in mind what you think your data should look like.\n",
    "- Verify that the data meets these expectations (or that you can explain why it doesn’t).\n",
    "- Double-check that the training data agrees with other sources (for example, dashboards).\n",
    "- Treat your data with all the care that you would treat any mission-critical code. Good ML relies on good data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Crosses\n",
    "\n",
    "A __feature cross__ is a __synthetic feature__ formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.\n",
    "\n",
    "### Feature Crosses: Encoding Nonlinearity\n",
    "\n",
    "Given a linear problem we classify class A and B using a line. What if we can't classify our data using a line? What if it is a non- linear problem?\n",
    "\n",
    "A feature cross is a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together. (The term cross comes from cross product.) Let's create a feature cross named `x3`  by crossing `x1` and `x2`:\n",
    "                    `x3 = x1x2`\n",
    "\n",
    "We treat this newly minted `x3` feature cross just like any other feature. The linear formula becomes:\n",
    "\n",
    "                `y = b+w1x1+w2x2+w3x3`\n",
    "\n",
    "A linear algorithm can learn a weight for `w3` just as it would for `w1` and `w2` . In other words, although `w3`  encodes nonlinear information, you don’t need to change how the linear model trains to determine the value of `w3`.\n",
    "\n",
    "__Kinds of feature crosses__\n",
    "We can create many different kinds of feature crosses. For example:\n",
    "\n",
    "- [A X B]: a feature cross formed by multiplying the values of two features.\n",
    "- [A x B x C x D x E]: a feature cross formed by multiplying the values of five features.\n",
    "- [A x A]: a feature cross formed by squaring a single feature.\n",
    "\n",
    "Thanks to __stochastic gradient descent__, linear models can be trained efficiently. Consequently, supplementing scaled linear models with feature crosses has traditionally been an efficient way to train on massive-scale data sets.\n",
    "\n",
    "### Feature Crosses: Crossing One-Hot Vectors\n",
    "\n",
    "So far, we've focused on feature-crossing two individual floating-point features. In practice, machine learning models seldom cross continuous features. However, machine learning models do frequently cross one-hot feature vectors. Think of feature crosses of one-hot feature vectors as logical conjunctions. For example, suppose we have two features: `country` and `language`. A one-hot encoding of each generates vectors with binary features that can be interpreted as `country=USA, country=France` or `language=English, language=Spanish`. Then, if you do a feature cross of these one-hot encodings, you get binary features that can be interpreted as logical conjunctions, such as:\n",
    "\n",
    "` country:usa AND language:spanish`\n",
    "\n",
    "As another example, suppose you bin latitude and longitude, producing separate one-hot five-element feature vectors. For instance, a given latitude and longitude could be represented as follows:\n",
    "\n",
    "` binned_latitude = [0, 0, 0, 1, 0]\n",
    "  binned_longitude = [0, 1, 0, 0, 0]`\n",
    "\n",
    "Suppose you create a feature cross of these two feature vectors:\n",
    "\n",
    "`binned_latitude X binned_longitude`\n",
    "\n",
    "This feature cross is a 25-element one-hot vector (24 zeroes and 1 one). The single `1` in the cross identifies a particular conjunction of latitude and longitude. Your model can then learn particular associations about that conjunction.\n",
    "\n",
    "Suppose we bin latitude and longitude much more coarsely, as follows:\n",
    "`\n",
    "binned_latitude(lat) = [\n",
    "  0  < lat <= 10\n",
    "  10 < lat <= 20\n",
    "  20 < lat <= 30\n",
    "]\n",
    "\n",
    "binned_longitude(lon) = [\n",
    "  0  < lon <= 15\n",
    "  15 < lon <= 30\n",
    "]\n",
    "`\n",
    "Creating a feature cross of those coarse bins leads to synthetic feature having the following meanings:\n",
    "`\n",
    "binned_latitude_X_longitude(lat, lon) = [\n",
    "  0  < lat <= 10 AND 0  < lon <= 15\n",
    "  0  < lat <= 10 AND 15 < lon <= 30\n",
    "  10 < lat <= 20 AND 0  < lon <= 15\n",
    "  10 < lat <= 20 AND 15 < lon <= 30\n",
    "  20 < lat <= 30 AND 0  < lon <= 15\n",
    "  20 < lat <= 30 AND 15 < lon <= 30\n",
    "]\n",
    "`\n",
    "Now suppose our model needs to predict how satisfied dog owners will be with dogs based on two features:\n",
    "\n",
    "- Behavior type (barking, crying, snuggling, etc.)\n",
    "- Time of day\n",
    "If we build a feature cross from both these features:\n",
    "`[behavior type X time of day]`\n",
    "\n",
    "then we'll end up with vastly more predictive ability than either feature on its own. For example, if a dog cries (happily) at 5:00 pm when the owner returns from work will likely be a great positive predictor of owner satisfaction. Crying (miserably, perhaps) at 3:00 am when the owner was sleeping soundly will likely be a strong negative predictor of owner satisfaction.\n",
    "\n",
    "Linear learners scale well to massive data. Using feature crosses on massive data sets is one efficient strategy for learning highly complex models. Neural networks provide another strategy.\n",
    "\n",
    "then we'll end up with vastly more predictive ability than either feature on its own. For example, if a dog cries (happily) at 5:00 pm when the owner returns from work will likely be a great positive predictor of owner satisfaction. Crying (miserably, perhaps) at 3:00 am when the owner was sleeping soundly will likely be a strong negative predictor of owner satisfaction.\n",
    "\n",
    "Linear learners scale well to massive data. Using feature crosses on massive data sets is one efficient strategy for learning highly complex models. Neural networks provide another strategy.\n",
    "\n",
    "__Q__ Different cities in California have markedly different housing prices. Suppose you must create a model to predict housing prices. Which of the following sets of features or feature crosses could learn city-specific relationships between roomsPerPerson and housing price?\n",
    "\n",
    "__Ans__ One feature cross: `[binned latitude X binned longitude X binned roomsPerPerson]`\n",
    "Crossing binned latitude with binned longitude enables the model to learn city-specific effects of roomsPerPerson. Binning prevents a change in latitude producing the same result as a change in longitude. Depending on the granularity of the bins, this feature cross could learn city-specific or neighborhood-specific or even block-specific effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for Simplicity\n",
    "\n",
    "__Regularization__ means penalizing the complexity of a model to reduce overfitting.\n",
    "\n",
    "### Regularization for Simplicity: L₂ Regularization\n",
    "\n",
    "Consider the a __generalization curve__, which shows the loss for both the training set and validation set against the number of training iterations.\n",
    "\n",
    "The model has training loss and validation loss, training loss  in which training loss gradually decreases, but validation loss eventually goes up. In other words, this generalization curve shows that the model is __overfitting__ to the data in the training set. Channeling our inner Ockham, perhaps we could prevent overfitting by penalizing complex models, a principle called __regularization__.\n",
    "\n",
    "In other words, instead of simply aiming to minimize loss (empirical risk minimization):\n",
    "                `minimize(Loss(Data|Model))`\n",
    "we'll now minimize loss+complexity, which is called __structural risk minimization__:\n",
    "       `minimize(Loss(Data|Model)+complexity(Model))`\n",
    "\n",
    "Our training optimization algorithm is now a function of two terms: the __loss term__, which measures how well the model fits the data, and the __regularization term__, which measures model complexity.\n",
    "\n",
    "Machine Learning Crash Course focuses on two common (and somewhat related) ways to think of model complexity:\n",
    "\n",
    "- Model complexity as a function of the weights of all the features in the model.\n",
    "- Model complexity as a function of the total number of features with nonzero weights. (A later module covers this approach.)\n",
    "\n",
    "If model complexity is a function of weights, a feature weight with a high absolute value is more complex than a feature weight with a low absolute value.\n",
    "\n",
    "We can quantify complexity using the __L2 regularization__ formula, which defines the regularization term as the sum of the squares of all the feature weights:\n",
    "   `L2 regularization term = ||w||^2 = w1^2+w2^2+...wn^2`\n",
    "\n",
    "In this formula, weights close to zero have little effect on model complexity, while outlier weights can have a huge impact.\n",
    "\n",
    "For example, a linear model with the following weights:\n",
    "\n",
    "{w1 = 0.2, w2= 0.5, __w3__= 5, w4= 1, w5= 0.25, w6 = 0.75 }\n",
    "\n",
    "Has an L2 regularization term of 26.915:\n",
    "\n",
    "But w3 (bolded above), with a squared value of 25, contributes nearly all the complexity. The sum of the squares of all five other weights adds just 1.915 to the L2 regularization term.\n",
    "\n",
    "Model developers tune the overall impact of the regularization term by multiplying its value by a scalar known as __lambda__ (also called the regularization rate). That is, model developers aim to do the following:\n",
    "       `minimize(Loss(Data|Model)+lambda complexity(Model))`\n",
    "Performing L2 regularization has the following effect on a model\n",
    "\n",
    "- Encourages weight values toward 0 (but not exactly 0)\n",
    "- Encourages the mean of the weights toward 0, with a normal (bell-shaped or Gaussian) distribution.\n",
    "\n",
    "Increasing the lambda value strengthens the regularization effect.\n",
    "\n",
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit:\n",
    "\n",
    "- If your lambda value is too high, your model will be simple, but you run the risk of _underfitting_ your data. Your model won't learn enough about the training data to make useful predictions.\n",
    "\n",
    "- If your lambda value is too low, your model will be more complex, and you run the risk of _overfitting_ your data. Your model will learn too much about the particularities of the training data, and won't be able to generalize to new data.\n",
    "\n",
    "___Setting lambda to zero removes regularization completely. In this case, training focuses exclusively on minimizing loss, which poses the highest possible overfitting risk.___\n",
    "\n",
    "The ideal value of lambda produces a model that generalizes well to new, previously unseen data. Unfortunately, that ideal value of lambda is data-dependent, so you'll need to do some tuning.\n",
    "\n",
    "__Q__ Imagine a linear model with 100 input features:\n",
    "- 10 are highly informative.\n",
    "- 90 are non-informative.\n",
    "Assume that all features have values between -1 and 1. Which of the following statements are true?\n",
    "\n",
    "__Ans__ \n",
    "- L2 regularization may cause the model to learn a moderate weight for some non-informative features.\n",
    "    Surprisingly, this can happen when a non-informative feature happens to be correlated with the label. In this case, the model incorrectly gives such non-informative features some of the \"credit\" that should have gone to informative features.\n",
    "- L2 regularization will encourage many of the non-informative weights to be nearly (but not exactly) 0.0.\n",
    "    Yes, L2 regularization encourages weights to be near 0.0, but not exactly 0.0.\n",
    "\n",
    "__Q__ Imagine a linear model with two strongly correlated features; that is, these two features are nearly identical copies of one another but one feature contains a small amount of random noise. If we train this model with L2 regularization, what will happen to the weights for these two features?\n",
    "\n",
    "__Ans__ Both features will have roughly equal, moderate weights.L2 regularization will force the features towards roughly equivalent weights that are approximately half of what they would have been had only one of the two features been in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Instead of predicting exactly 0 or 1, logistic regression generates a probability—a value between 0 and 1, exclusive. For example, consider a logistic regression model for spam detection. If the model infers a value of 0.932 on a particular email message, it implies a 93.2% probability that the email message is spam. More precisely, it means that in the limit of infinite training examples, the set of examples for which the model predicts 0.932 will actually be spam 93.2% of the time and the remaining 6.8% will not.\n",
    "\n",
    "### Logistic Regression: Calculating a Probability\n",
    "\n",
    "Many problems require a probability estimate as output. Logistic regression is an extremely efficient mechanism for calculating probabilities. Practically speaking, you can use the returned probability in either of the following two ways:\n",
    "\n",
    "- \"As is\"\n",
    "- Converted to a binary category.\n",
    "\n",
    "Let's consider how we might use the probability \"as is.\" Suppose we create a logistic regression model to predict the probability that a dog will bark during the middle of the night. We'll call that probability:\n",
    "`p(bark|night)`\n",
    "If the logistic regression model predicts a `p(bark | night)` of 0.05, then over a year, the dog's owners should be startled awake approximately 18 times:\n",
    "`\n",
    "startled = p(bark|night).nights\n",
    "           =0.05.365\n",
    "           = 18\n",
    "           `\n",
    "In many cases, you'll map the logistic regression output into the solution to a binary classification problem, in which the goal is to correctly predict one of two possible labels (e.g., \"spam\" or \"not spam\").\n",
    "\n",
    "You might be wondering how a logistic regression model can ensure output that always falls between 0 and 1. As it happens, a sigmoid function, defined as follows, produces output having those same characteristics:  \n",
    "                 `y' = 1/1+exp(-z)`\n",
    "Here \n",
    "- z: the output of the linear layer of a model trained with logit regression\n",
    "- y': the output of logistic regression for a particular example\n",
    "- z = b+w1x1+w2x2+w3x3+......+wnxn\n",
    "    - w values are the model's learned weights\n",
    "    - b is bias\n",
    "    - x values for particular example\n",
    "    \n",
    "Note that `z` is also referred to as the log-odds because the inverse of the sigmoid states that `z` can be defined as the log of the probability of the \"1\" label (e.g., \"dog barks\") divided by the probability of the \"0\" label (e.g., \"dog doesn't bark\"):\n",
    "            `z = log(y/1-y)` \n",
    "            \n",
    "## Loss function for Logistic Regression\n",
    "\n",
    "The loss function for linear regression is squared loss. The loss function for logistic regression is Log Loss, which is defined as follows:\n",
    "\n",
    "`Log Loss = Sum(-ylog(y')-(1-y)log(1-y')`\n",
    "where:\n",
    "- y is the label in a labeled example. Since this is logistic regression, every value of y must either be 0 or 1.\n",
    "- y' is the predicted value (somewhere between 0 and 1), given the set of features in x.\n",
    "\n",
    "### Regularization in Logistic Regression\n",
    "Regularization is extremely important in logistic regression modeling. Without regularization, the __asymptotic__ nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "\n",
    "### L2 regularization.\n",
    "Early stopping, that is, limiting the number of training steps or the learning rate.\n",
    "(We'll discuss a third strategy—L1 regularization—in a later module.)\n",
    "\n",
    "Imagine that you assign a unique id to each example, and map each id to its own feature. If you don't specify a regularization function, the model will become completely overfit. That's because the model would try to drive loss to zero on all examples and never get there, driving the weights for each indicator feature to +infinity or -infinity. This can happen in high dimensional data with feature crosses, when there’s a huge mass of rare crosses that happen only on one example each.\n",
    "\n",
    "Fortunately, using L2 or early stopping will prevent this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
