{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Crash Course\n",
    "resource: https://developers.google.com/machine-learning/crash-course/ml-intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing\n",
    "\n",
    "__What is (supervised) machine learning? Concisely put, it is the following:\n",
    "\n",
    "- ML systems learn how to combine input to produce useful predictions on never-before-seen data.\n",
    "\n",
    "Let's explore fundamental machine learning terminology.\n",
    "\n",
    "__Labels__\n",
    "A label is the thing we're predicting—the y variable in simple linear regression. The label could be the future price of wheat, the kind of animal shown in a picture, the meaning of an audio clip, or just about anything.\n",
    "\n",
    "__Features__\n",
    "A feature is an input variable—the x variable in simple linear regression. A simple machine learning project might use a single feature, while a more sophisticated machine learning project could use millions of features, specified as:\n",
    "x1,x2......xn\n",
    "\n",
    "In the spam detector example, the features could include the following:\n",
    "\n",
    "- words in the email text\n",
    "- sender's address\n",
    "- time of day the email was sent\n",
    "- email contains the phrase \"one weird trick.\"\n",
    "\n",
    "An example is a particular instance of data, x. (We put x in boldface to indicate that it is a vector.) We break examples into two categories:\n",
    "\n",
    "- labeled examples\n",
    "- unlabeled examples\n",
    "\n",
    "A labeled example includes both feature(s) and the label. That is:\n",
    "\n",
    "labeled examples: {features, label}: (x, y)\n",
    "\n",
    "_Use labeled examples to train the model. In our spam detector example, the labeled examples would be individual emails that users have explicitly marked as \"spam\" or \"not spam.\"_\n",
    "\n",
    "An unlabeled example contains features but not the label. That is:\n",
    "\n",
    "unlabeled examples: {features, ?}: (x, ?)\n",
    "\n",
    "__Models__\n",
    "\n",
    "A model defines the relationship between features and label. For example, a spam detection model might associate certain features strongly with \"spam\". Let's highlight two phases of a model's life:\n",
    "\n",
    "Training means creating or learning the model. That is, you show the model labeled examples and enable the model to gradually learn the relationships between features and label.\n",
    "\n",
    "Inference(testing) means applying the trained model to unlabeled examples. That is, you use the trained model to make useful predictions (y'). For example, during inference(testing), you can predict medianHouseValue for new unlabeled examples.\n",
    "\n",
    "__Regression vs. classification__\n",
    "A regression model predicts continuous values. For example, regression models make predictions that answer questions like the following:\n",
    "\n",
    "- What is the value of a house in California?\n",
    "\n",
    "- What is the probability that a user will click on this ad?\n",
    "\n",
    "A classification model predicts discrete values. For example, classification models make predictions that answer questions like the following:\n",
    "\n",
    "- Is a given email message spam or not spam?\n",
    "\n",
    "- Is this an image of a dog, a cat, or a hamster?\n",
    "\n",
    "### Check your Understanding:\n",
    "\n",
    "01. Suppose you want to develop a supervised machine learning model to predict whether a given email is \"spam\" or \"not spam.\" Which of the following statements are true?\n",
    "\n",
    " - Emails not marked as \"spam\" or \"not spam\" are unlabeled examples.\n",
    " - The labels applied to some examples might be unreliable.\n",
    "\n",
    "02. Suppose an online shoe store wants to create a supervised ML model that will provide personalized shoe recommendations to users. That is, the model will recommend certain pairs of shoes to Marty and different pairs of shoes to Janet. The system will use past user behavior data to generate training data. Which of the following statements are true?\n",
    "\n",
    " - \"The user clicked on the shoe's description\" is a useful label.\n",
    " - \"Shoe size\" is a useful feature.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descending into ML\n",
    "\n",
    "__Linear regression__ is a method for finding the straight line or hyperplane that best fits a set of points. This module explores linear regression intuitively before laying the groundwork for a machine learning approach to linear regression.\n",
    "\n",
    "Linear Regression is the line of best fit among the data points present the line doesn't pass through every dot, but the line does clearly show the relationship between x and y. Using the equation for a line, you could write down this relationship as follows:\n",
    "\n",
    "where: \n",
    "\n",
    "y = mx+c\n",
    "\n",
    "- y is the the value we're trying to predict.\n",
    "- m is the slope of the line.\n",
    "- x is the  value of our input feature.\n",
    "- c is the y-intercept.\n",
    "By convention in machine learning, you'll write the equation for a model slightly differently:\n",
    "\n",
    "where:\n",
    "\n",
    "y' = w1x1 + c\n",
    "\n",
    "- y' is the predicted label (a desired output).\n",
    "- c is the bias (the y-intercept), sometimes referred to as w0.\n",
    "- w1 is the weight of feature 1. Weight is the same concept as the \"slope\"  in the traditional equation of a line.\n",
    "- x1 is a feature (a known input).\n",
    "To infer (predict) the a new y' value , just substitute the  value into this model.\n",
    "\n",
    "Although this model uses only one feature, a more sophisticated model might rely on multiple features, each having a separate weight (w1,w2 , etc.). For example, a model that relies on three features might look as follows:\n",
    "\n",
    "y' = w1x1+w2x2+c\n",
    "\n",
    "### Descending into ML: Training and Loss\n",
    "\n",
    "__Training__ a model simply means learning (determining) good values for all the weights and the bias from labeled examples. In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss; this process is called __empirical risk minimization.__\n",
    "\n",
    "__Loss is the penalty for a bad prediction.__ That is, loss is a number indicating how bad the model's prediction was on a single example. If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater. The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples.\n",
    "\n",
    "__Squared loss: a popular loss function__\n",
    "The linear regression models we'll examine here use a loss function called squared loss (also known as L2 loss). The squared loss for a single example is as follows:\n",
    "\n",
    " = the square of the difference between the label and the prediction\n",
    " \n",
    " = (observation - prediction(x))2\n",
    " \n",
    " = (y - y')2\n",
    " \n",
    "__Mean square error (MSE)__ is the average squared loss per example over the whole dataset. To calculate MSE, sum up all the squared losses for individual examples and then divide by the number of examples.\n",
    "\n",
    "\n",
    "### Check your understanding\n",
    "\n",
    "01. Which of the two data sets shown in the preceding plots has the higher Mean Squared Error (MSE)?\n",
    "    - The dataset on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Loss\n",
    "\n",
    "To train a model, we need a good way to reduce the model’s loss. An iterative approach is one widely used method for reducing loss, and is as easy and efficient as walking down a hill.\n",
    "\n",
    "### Reducing Loss: An Iterative Approach\n",
    "\n",
    "The previous module introduced the concept of loss. Here, in this module, you'll learn how a machine learning model iteratively reduces loss.\n",
    "\n",
    "Iterative learning might remind you of the \"Hot and Cold\" kid's game for finding a hidden object like a thimble. In this game, the \"hidden object\" is the best possible model. You'll start with a wild guess (\"The value of w1 is 0.\") and wait for the system to tell you what the loss is. Then, you'll try another guess (\"The value of w1 is 0.5.\") and see what the loss is. Aah, you're getting warmer. Actually, if you play this game right, you'll usually be getting warmer. The real trick to the game is trying to find the best possible model as efficiently as possible.\n",
    "\n",
    "We'll use this same iterative approach throughout the Machine Learning Crash Course, detailing various complications, particularly within that stormy cloud labeled \"Model (Prediction Function).\" Iterative strategies are prevalent in machine learning, primarily because they scale so well to large data sets.\n",
    "\n",
    "The \"model\" takes one or more features as input and returns one prediction (y') as output. To simplify, consider a model that takes one feature and returns one prediction:\n",
    "                                \n",
    "                                y' = b+w1x1\n",
    "\n",
    "What initial values should we set for  and ? For linear regression problems, it turns out that the starting values aren't important. We could pick random values, but we'll just take the following trivial values instead:\n",
    "\n",
    "b = 0\n",
    "w1 = 0\n",
    "Suppose that the first feature value is 10. Plugging that feature value into the prediction function yields:\n",
    "\n",
    "                                y' = 0+0.10\n",
    "\n",
    "The \"Compute Loss\" part of the diagram is the loss function that the model will use. Suppose we use the squared loss function. The loss function takes in two input values:\n",
    "                            Loss = (y-y')^2\n",
    "\n",
    "y': The model's prediction for features x\n",
    "\n",
    "y: The correct label corresponding to features x.\n",
    "\n",
    "At last, we've reached the \"Compute parameter updates\" part of the diagram. It is here that the machine learning system examines the value of the loss function and generates new values for  and . For now, just assume that this mysterious box devises new values and then the machine learning system re-evaluates all those features against all those labels, yielding a new value for the loss function, which yields new parameter values. And the learning continues iterating until the algorithm discovers the model parameters with the lowest possible loss. Usually, you iterate until overall loss stops changing or at least changes extremely slowly. When that happens, we say that the model has converged.\n",
    "\n",
    "___A Machine Learning model is trained by starting with an initial guess for the weights and bias and iteratively adjusting those guesses until learning the weights and bias with the lowest possible loss.___\n",
    "\n",
    "### Reducing Loss: Gradient Descent\n",
    "\n",
    "The iterative approach contained a green hand-wavy box entitled \"Compute parameter updates.\" We'll now replace that algorithmic fairy dust with something more substantial.\n",
    "\n",
    "Suppose we had the time and the computing resources to calculate the loss for all possible values of w1. For the kind of regression problems we've been examining, the resulting plot of loss vs.w1 will always be convex. In other words, the plot will always be bowl-shaped.\n",
    "\n",
    "Convex problems have only one minimum; that is, only one place where the slope is exactly 0. That minimum is where the loss function converges.\n",
    "\n",
    "Calculating the loss function for every conceivable value of  over the entire data set would be an inefficient way of finding the convergence point. Let's examine a better mechanism—very popular in machine learning—called __gradient descent.__\n",
    "\n",
    "The first stage in gradient descent is to pick a starting value (a starting point) for w1. The starting point doesn't matter much; therefore, many algorithms simply set w1 to 0 or pick a random value. \n",
    "\n",
    "The gradient descent algorithm then calculates the gradient of the loss curve at the starting point. The gradient of the loss is equal to the derivative (slope) of the curve, and tells you which way is \"warmer\" or \"colder.\" When there are multiple weights, the gradient is a vector of partial derivatives with respect to the weights.\n",
    "\n",
    "Note that a gradient is a vector, so it has both of the following characteristics:\n",
    "\n",
    "- a direction\n",
    "- a magnitude\n",
    "\n",
    "The gradient always points in the direction of steepest increase in the loss function. The gradient descent algorithm takes a step in the direction of the negative gradient in order to reduce loss as quickly as possible.\n",
    "\n",
    "The gradient descent then repeats this process, edging ever closer to the minimum.\n",
    "\n",
    "___When performing gradient descent, we generalize the above process to tune all the model parameters simultaneously. For example, to find the optimal values of both w1 and the bias b , we calculate the gradients with respect to both w1 and b . Next, we modify the values of w1 and b based on their respective gradients. Then we repeat these steps until we reach minimum loss.___\n",
    "\n",
    "### Reducing Loss: Learning Rate\n",
    "\n",
    "As noted, the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the __learning rate__ (also sometimes called step size) to determine the next point. For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.\n",
    "\n",
    "__Hyperparameters__ are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. If you pick a learning rate that is too small, learning will take too long\n",
    "\n",
    "Conversely, if you specify a learning rate that is too large, the next point will perpetually bounce haphazardly across the bottom of the well like a quantum mechanics experiment gone horribly wrong\n",
    "\n",
    "There's a Goldilocks learning rate for every regression problem. The Goldilocks value is related to how flat the loss function is. If you know the gradient of the loss function is small then you can safely try a larger learning rate, which compensates for the small gradient and results in a larger step size.\n",
    "\n",
    "### Reducing Loss: Optimizing Learning Rate\n",
    "\n",
    "___In practice, finding a \"perfect\" (or near-perfect) learning rate is not essential for successful model training. The goal is to find a learning rate large enough that gradient descent converges efficiently, but not so large that it never converges.___\n",
    "\n",
    "### Reducing Loss: Stochastic Gradient Descent\n",
    "\n",
    "In gradient descent, a __batch__ is the total number of examples you use to calculate the gradient in a single iteration. So far, we've assumed that the batch has been the entire data set. When working at Google scale, data sets often contain billions or even hundreds of billions of examples. Furthermore, Google data sets often contain huge numbers of features. Consequently, a batch can be enormous. A very large batch may cause even a single iteration to take a very long time to compute.\n",
    "\n",
    "A large data set with randomly sampled examples probably contains redundant data. In fact, redundancy becomes more likely as the batch size grows. Some redundancy can be useful to smooth out noisy gradients, but enormous batches tend not to carry much more predictive value than large batches.\n",
    "\n",
    "What if we could get the right gradient on average for much less computation? By choosing examples at random from our data set, we could estimate (albeit, noisily) a big average from a much smaller one. Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\n",
    "\n",
    "__Mini-batch stochastic gradient descent__ (mini-batch SGD) is a compromise between full-batch iteration and SGD. A mini-batch is typically between 10 and 1,000 examples, chosen at random. Mini-batch SGD reduces the amount of noise in SGD but is still more efficient than full-batch.\n",
    "\n",
    "To simplify the explanation, we focused on gradient descent for a single feature. Rest assured that gradient descent also works on feature sets that contain multiple features.\n",
    "\n",
    "### Reducing Loss: Check Your Understanding\n",
    "\n",
    "01. When performing gradient descent on a large data set, which of the following batch sizes will likely be more efficient?\n",
    "    - A small batch or even a batch of one example (SGD).\n",
    "     Amazingly enough, performing gradient descent on a small batch or even a batch of one example is usually more efficient than the full batch. After all, finding the gradient of one example is far cheaper than finding the gradient of millions of examples. To ensure a good representative sample, the algorithm scoops up another random small batch (or batch of one) on every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
