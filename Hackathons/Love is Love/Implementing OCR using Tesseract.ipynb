{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Love is Love Hackathon\n",
    "\n",
    "### Problem statement\n",
    "\n",
    "Love knows no gender and the LGBTQ (Lesbian, Gay, Bisexual, Transgender, and Queer) community is the epitome of this thought. During Pride Month, we(Hacker Earth) are here with another Machine Learning challenge to celebrate the impact and changes that they made globally.\n",
    "\n",
    "You have been appointed as a social media moderator for your firm. Your key responsibility is to tag and categorize quotes that are uploaded during Pride Month on the basis of its sentiment, positive, negative, and random. Your task is to build a sophisticated Machine Learning model combining Optical Character Recognition (OCR) and Natural Language Processing (NLP) to assess sentiments of these quotes.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset consists of quotes that are uploaded during Pride Month.\n",
    "\n",
    "The benefits of practicing this problem by using unsupervised Machine Learning techniques are as follows:\n",
    "\n",
    "This challenge encourages you to apply your unsupervised Machine Learning skills to build models that can assess sentiments of a quote.\n",
    "This challenge helps you enhance your knowledge of OCR and NLP that are a part of the advanced fields of Machine Learning and artificial intelligence.\n",
    "You are required to build a model that analyzes sentiments of a quote and classifies them into positive, negative, or random\n",
    "\n",
    "### Detecting sentiments of a quote\n",
    "\n",
    "You work as a social media moderator for your firm. Your key responsibility is to tag uploaded content (images) during Pride Month based on its sentiment (positive, negative, or random) and categorize them for internal reference and SEO optimization.\n",
    "\n",
    "### Task\n",
    "Your task is to build an engine that combines the concepts of OCR and NLP that accepts a .jpg file as input, extracts the text, if any, and classifies sentiment as positive or negative. If the text sentiment is neutral or an image file does not have any text, then it is classified as random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OCR implementation using Tesseract\n",
    "\n",
    "#### What is OCR?\n",
    "\n",
    "OCR is Optical Charcater Recgonition,Optical Character Recognition involves the detection of text content on images and translation of the images to encoded text that the computer can easily understand. An image containing text is scanned and analyzed in order to identify the characters in it. Upon identification, the character is converted to machine-encoded text.\n",
    "\n",
    "__How is it really achieved?__ To us, text on an image is easily discernible and we are able to detect characters and read the text, but to a computer, it is all a series of dots.\n",
    "\n",
    "The image is first scanned and the text and graphics elements are converted into a bitmap, which is essentially a matrix of black and white dots. The image is then pre-processed where the brightness and contrast are adjusted to enhance the accuracy of the process.\n",
    "\n",
    "The image is now split into zones identifying the areas of interest such as where the images or text are and this helps kickoff the extraction process. The areas containing text can now be broken down further into lines and words and characters and now the software is able to match the characters through comparison and various detection algorithms. The final result is the text in the image that we're given.\n",
    "\n",
    "__For this OCR project, I used the Python-Tesseract, or simply PyTesseract, library which is a wrapper for Google's Tesseract-OCR Engine.__\n",
    "\n",
    "The Python-Tesseract is downloaded from the following location:\n",
    "https://tesseract-ocr.github.io/tessdoc/\n",
    "\n",
    "Pytesseract is a wrapper for Tesseract-OCR Engine. It is also useful as a stand-alone invocation script to tesseract, as it can read all image types supported by the Pillow and Leptonica imaging libraries, including jpeg, png, gif, bmp, tiff, and others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling tesseract from the location\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemening Tesseract on a simple image without any pre-processing steps\n",
    "img = cv2.imread(\"Test803.jpg\")\n",
    "cv2.imshow(\"Img\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('Img') #Added this statement after my jupyter notebook was crashing after imshow statement \n",
    "#Error:\n",
    "#error: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:376: \n",
    "#error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        ...,\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248]],\n",
       "\n",
       "       [[198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        ...,\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247]],\n",
       "\n",
       "       [[197, 239, 246],\n",
       "        [197, 239, 246],\n",
       "        [197, 239, 246],\n",
       "        ...,\n",
       "        [197, 239, 246],\n",
       "        [197, 239, 246],\n",
       "        [197, 239, 246]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[196, 238, 245],\n",
       "        [196, 238, 245],\n",
       "        [196, 238, 245],\n",
       "        ...,\n",
       "        [196, 238, 245],\n",
       "        [196, 238, 245],\n",
       "        [196, 238, 245]],\n",
       "\n",
       "       [[198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        ...,\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247],\n",
       "        [198, 240, 247]],\n",
       "\n",
       "       [[199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        ...,\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248],\n",
       "        [199, 241, 248]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =pytesseract.image_to_string(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Certain portion of the text is correct but let's see if there is any improvement in the image after converting the image to gray scale image\n",
    "#Grayscaling the image via OpenCV\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "cv2.imshow(\"gray\",gray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text_gray =pytesseract.image_to_string(gray)\n",
    "print(text_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise removal\n",
    "def remove_noise(image):\n",
    "    return cv2.medianBlur(image,5)\n",
    "img_rm =remove_noise(gray)\n",
    "cv2.imshow(\"img_rm\",img_rm)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('img_rm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&\n",
      "\n",
      "Aim origanal 4 abmemagy\n",
      "wer th mare Shan a cleus\n"
     ]
    }
   ],
   "source": [
    "text_rm =pytesseract.image_to_string(img_rm)\n",
    "print(text_rm)\n",
    "#Outcome of the image is better when the noise removal is done on gray image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fe\n",
      "\n",
      "An opine,’ y edema\n",
      "eer Th mere Phan © sige\n"
     ]
    }
   ],
   "source": [
    "def thresholding(image):\n",
    "    return cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "img_th =thresholding(img_rm)\n",
    "cv2.imshow(\"img_th\",img_th)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyWindow('img_th')\n",
    "text_th =pytesseract.image_to_string(img_th)\n",
    "print(text_th)\n",
    "#the results noise removed image + image threshold gives better results \n",
    "#but so far noise removed + gray image has better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dilation \n",
    "#Dilation adds pixels to the boundaries of objects in an image,\n",
    "def dilate(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.dilate(image, kernel, iterations = 1)\n",
    "img_dil =dilate(img_rm)\n",
    "cv2.imshow(\"img_dil\",img_dil)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('img_dil')\n",
    "text_dil =pytesseract.image_to_string(img_dil)\n",
    "print(text_dil)\n",
    "#Text in image is hardly visible after dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#erosion\n",
    "def erode(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.erode(image, kernel, iterations = 1)\n",
    "img_er =erode(img_rm)\n",
    "cv2.imshow(\"img_er\",img_er)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('img_er')\n",
    "text_er =pytesseract.image_to_string(img_er)\n",
    "print(text_er)\n",
    "# so far noise removed + gray image has better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we hen ohy\n"
     ]
    }
   ],
   "source": [
    "#opening - erosion followed by dilation\n",
    "def opening(image):\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    return cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\n",
    "img_op =erode(gray)\n",
    "cv2.imshow(\"img_op\",img_op)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('img_op')\n",
    "text_op =pytesseract.image_to_string(img_op)\n",
    "print(text_op)\n",
    "# so far noise removed + gray image has better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pe\n",
      "\n",
      "7D aa\n",
      "pave Scan\n",
      "Aue og ee\n"
     ]
    }
   ],
   "source": [
    "#canny edge detection\n",
    "def canny(image):\n",
    "    return cv2.Canny(image, 100, 200)\n",
    "img_can =canny(img)\n",
    "cv2.imshow(\"img_can\",img_can)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('img_can')\n",
    "text_can =pytesseract.image_to_string(img_can)\n",
    "print(text_can)\n",
    "\n",
    "# so far noise removed + gray image has better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yess\n",
      "\n",
      "i\n",
      "Pe oy ie\n"
     ]
    }
   ],
   "source": [
    "#skew correction\n",
    "def deskew(image):\n",
    "    coords = np.column_stack(np.where(image > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return rotated\n",
    "img_ds =canny(gray)\n",
    "cv2.imshow(\"img_ds\",img_ds)\n",
    "cv2.waitKey()\n",
    "cv2.destroyWindow('img_ds')\n",
    "text_ds =pytesseract.image_to_string(img_ds)\n",
    "print(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
