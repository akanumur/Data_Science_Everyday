{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulid models that can process text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word based encodings\n",
    "The Ascii values just gives the information or numbers for the letters in the word. The words SILENT and LISTEN have the same codes.\n",
    "\n",
    "So that 's the reason we try giving the sentence an encoding instead of the word. \n",
    "For Example: I love my dog gets codes as 01 02 03 04\n",
    "but when we try I love my cat it gives same numbers for I love my and cat gets 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "#Using APIs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = ['I love my dog','I love my cat']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100) #num_words gives the top 100 unique words by volume in cases we have huge amount of data\n",
    "tokenizer.fit_on_texts(sentences) # takes the data and encodes it\n",
    "word_index = tokenizer.word_index # gives key value pairs \n",
    "print(word_index)\n",
    "#It is very similar to fitting a ML model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are 5 unique words in the above two sentences when we give another new sentence it gives a new value\n",
    "Tokenizer strios punctuation I is shown as lower case \"i\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Text to sequence\n",
    "# New set of sentences\n",
    "sentences = ['I love my dog','I love my cat','you love my dog','Do you think my dog is amazing?']\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1], [1, 9]]\n"
     ]
    }
   ],
   "source": [
    "test_data = ['I love my name','My name is Akshata']\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)# We can see that the words which we were not identified in the output were skipped bcoz our train set did not contain that information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# In order to handle unknown data it is always a good practice to use oov parameter to replace unknown words in test set\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Text to sequence\n",
    "# New set of sentences\n",
    "sentences = ['I love my dog','I love my cat','you love my dog','Do you think my dog is amazing?']\n",
    "tokenizer = Tokenizer(num_words=100,oov_token =\"<OOV>\") # oov_token means Out of vocabulary\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that OOV takes a key of 1 now let us test on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 1], [2, 1, 10, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = ['I love my name','My name is Akshata']\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)# We can see that the words which we were not identified are replaced with OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n",
      "[[ 0  0  0  4  2  1  3]\n",
      " [ 0  0  0  4  2  1  6]\n",
      " [ 0  0  0  5  2  1  3]\n",
      " [ 7  5  8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# Padding : In order to maintain uniformity in size we use padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#Text to sequence\n",
    "# New set of sentences\n",
    "sentences = ['I love my dog','I love my cat','you love my dog','Do you think my dog is amazing?']\n",
    "tokenizer = Tokenizer(num_words=100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "print(word_index)\n",
    "print(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a matrix is created in order to convert the text data so that it can be used by our neural network\n",
    "The output of the lines which has fewer words than the longest sentences have 0's in the start \n",
    "usually padding =\"post\" if we want 0's in the last\n",
    "maxlen = n If we want the sentences to have n words\n",
    "using truncate we can change the from where we want to consider the sentence pre indicates start post indicates end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1], [1, 9]]\n",
      "\n",
      "post padding\n",
      "[[4 2 1 0 0 0 0]\n",
      " [1 9 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test_data = ['I love my name','My name is Akshata']\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)# We can see that the words which we were not identified are replaced with OOV\n",
    "padded = pad_sequences(test_seq , maxlen = 7, padding = \"post\")\n",
    "print(\"\\npost padding\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
