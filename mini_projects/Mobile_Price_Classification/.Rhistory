library(caret)
confusionMatrix(table(glm_pred, titan_test$Survival))
confusionMatrix(table(lda_pred$class, titan_test$Survival))
confusionMatrix(table(qda_pred$class, titan_test$Survival))
confusionMatrix(table(bayes_pred, titan_test$Survival))
confusionMatrix(table(knn.fit, titan_test$Survival))
# If you want to change the positive to Survived, add positive="Survived"
confusionMatrix(table(glm_pred, titan_test$Survival), positive="Survived")
# Details on each calculation are available in the documentation.
?confusionMatrix
confusionMatrix(table(glm_pred, titan_test$Survival), positive="Survived")
transaction1<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/Transactions_Part1.csv")
transaction1<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/Transactions_Part1.csv")
transaction2<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/Transactions_Part2.csv")
transaction <- rbind(transaction1, transaction2)
summary(transaction)
View(transaction)
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
claim<-read.xlsx("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.xlsx")
install.packages("xlsx")
claim<-read.xlsx("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.xlsx")
library.xlsx
library xlsx
library(xlsx)
install.packages("xlsx")
library(xlsx)
claim<-read.xlsx("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.xlsx")
claim<-read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Projects/Data/ClaimsDataFall2018.csv")
View(claim)
attach(transaction)
distinct(transaction$ClaimIdentifier)
unique(transaction$ClaimIdentifier)
count(unique(transaction$ClaimIdentifier))
a<-count(unique(transaction$ClaimIdentifier))
a<-(unique(transaction$ClaimIdentifier))
View(transaction)
actual<-c("p","p","p","p","n","n","p","p","n","n")
prob1<-c(0.9,0.8,0.7,0.65,0.6,0.55,0.5,0.45,0.4,0.35)
prob2<-c(0.85,0.8,0.75,0.65,0.55,0.45,0.4,0.35,0.3,0.25)
actual2<-c("p","p","p","p","n","p","n","n","p","n")
hwdata<-data.frame(actual,actual2,prob1, prob2)
hwdata
pred1<-prediction(hwdata$prob1, hwdata$actual)
install.packages("ROCR")
library(ROCR)
pred1<-prediction(hwdata$prob1, hwdata$actual)
pred2<-prediction(hwdata$prob2, hwdata$actual2)
perf1<-performance(pred1, measure="tpr",x.measure="fpr")
perf2<-performance(pred2, measure="tpr",x.measure="fpr")
plot(perf1,colorize=TRUE)
abline(a=0,b=1)
plot(perf2,add=TRUE, colorize=TRUE)
auc_obj1<-performance(pred1, "auc")
auc_obj1@y.values
auc_obj2<-performance(pred2, "auc")
auc_obj2@y.values
claimsData <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv")
claimsData <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv")
ncount <- nrow(claimsData)
ncount
View(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
text_df <- data_frame(line = 1:ncount, text = incidentData)
install.packages("tidytext")
install.packages("dplyr")
install.packages("ggplot2")
text_df <- data_frame(line = 1:ncount, text = incidentData)
library(tidytext)
library(dplyr)
library(ggplot2)
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, word, text)
View(text_df)
token_data <- unnest_tokens(text_df, word, text)
token_data <- anti_join(token_data, stop_words)
token_data <- unnest_tokens(text_df, word, text)
?unnest_tokens
token_data <- unnest_tokens(text_df, words, text)
token_data <- unnest_tokens(text_df)
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
text_df<-text_df[,2]
head(text_df)
token_data <- unnest_tokens(text_df)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, words, text)
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
library(tidytext)
library(dplyr)
library(ggplot2)
claimsData <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv")
ncount <- nrow(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
library(tidytext)
library(dplyr)
library(ggplot2)
install.packages(c("tm", "SnowballC", "wordcloud", "RColorBrewer"))
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
Jeopardy <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/JEOPARDY_CSV.csv")
Jeopardy <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/JEOPARDY_CSV.csv")
View(Jeopardy)
JQCorpus <- Corpus(VectorSource(Jeopardy$Question))
JQCorpus <- Corpus(VectorSource(Jeopardy$Question))
JQCorpus <- tm_map(JQCorpus, content_transformer(tolower))
JQCorpus <- tm_map(JQCorpus, PlainTextDocument)
JQCorpus <- tm_map(JQCorpus, removePunctuation)
JQCorpus <- tm_map(JQCorpus, removeWords, stopwords('english'))
JQCorpus <- tm_map(JQCorpus, stemDocument)
JQCorpus <- Corpus(VectorSource(JQCorpus))
wordcloud(JQCorpus, max.words = 25, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
wordcloud(JQCorpus, max.words = 100, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
JEOPARDY_CSV <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/JEOPARDY_CSV.csv", stringsAsFactors=FALSE)
View(JEOPARDY_CSV)
Jeopardy <- JEOPARDY_CSV
JQCorpus <- Corpus(VectorSource(Jeopardy$Question))
JQCorpus <- tm_map(JQCorpus, content_transformer(tolower))
JQCorpus <- tm_map(JQCorpus, removePunctuation)
JQCorpus <- tm_map(JQCorpus, PlainTextDocument)
JQCorpus <- tm_map(JQCorpus, removeWords, stopwords('english'))
JQCorpus <- tm_map(JQCorpus, stemDocument)
JQCorpus <- Corpus(VectorSource(JQCorpus))
wordcloud(JQCorpus, max.words = 100, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
# Word Cloud of Injury Description from Claims
# Source: https://datascienceplus.com/building-wordclouds-in-r/
# Source: http://onepager.togaware.com/TextMiningO.pdf
# install following packages, if they are not already installed
#   tm
#   SnowballC
#   wordcloud
#   RColorBrewer
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
# load the above packages
# read the csv file Jeopardy_CSV (download from Canvas before reading)
# while reading, UNCHECK the StringsAsFactors box
Jeopardy <- JEOPARDY_CSV
# the following commands convert the description column into corpus
# and use functions from tm package to prepare for the word cloud
JQCorpus <- Corpus(VectorSource(Jeopardy$Question))
JQCorpus <- tm_map(JQCorpus, content_transformer(tolower))
JQCorpus <- tm_map(JQCorpus, removePunctuation)
JQCorpus <- tm_map(JQCorpus, PlainTextDocument)
JQCorpus <- tm_map(JQCorpus, removeWords, stopwords('english'))
JQCorpus <- tm_map(JQCorpus, stemDocument)
JQCorpus <- Corpus(VectorSource(JQCorpus))
wordcloud(JQCorpus, max.words = 100, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
library("tidytext", lib.loc="~/R/win-library/3.5")
library("dplyr", lib.loc="~/R/win-library/3.5")
library("tidytext", lib.loc="~/R/win-library/3.5")
library("ggplot2", lib.loc="~/R/win-library/3.5")
library("tidytext", lib.loc="~/R/win-library/3.5")
JEOPARDY_Data <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Extra Credit/JEOPARDY_Data.csv", stringsAsFactors=FALSE)
View(JEOPARDY_Data)
ncount <- nrow(JData)
# store the data in a shorter named dataframe
JData <- JEOPARDY_Data
# count number of rows
ncount <- nrow(JData)
ncount
ncols(JData)
ncols(JData)
ncol(JData)
incidentData <- JData$Questions
head(JData)
# extract only the Questions column into a dataset
QuestionsData <- JData$Questions
# convert the data to a data frame
text_df <- data_frame(line = 1:ncount, text = QuestionsData)
JEOPARDY_Data <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Extra Credit/JEOPARDY_Data.csv", stringsAsFactors=FALSE)
View(JEOPARDY_Data)
library("tidytext", lib.loc="~/R/win-library/3.5")
library("tidytext", lib.loc="~/R/win-library/3.5")
install.packages("SnowballC")
library("tidytext", lib.loc="~/R/win-library/3.5")
JData <- JEOPARDY_Data
ncount <- nrow(JData)
ncount
QuestionsData <- JData$Questions
text_df <- data_frame(line = 1:ncount, text = QuestionsData)
head(text_df)
text_df <- data_frame(line = 1:ncount, text = QuestionsData)
QuestionsData <- JData$Questions
JData <- JEOPARDY_Data
View(JData)
QuestionsData <- JData$Question
text_df <- data_frame(line = 1:ncount, text = QuestionsData)
head(text_df)
View(text_df)
# tokenize with standard tokenization using unnest_tokens from tidytext
token_data <- unnest_tokens(text_df, words, text)
# remove stop-words using anti_join function from dplyr
# stop_words come from tidytext package
token_data <- anti_join(token_data, stop_words)
# tokenize with standard tokenization using unnest_tokens from tidytext
token_data <- unnest_tokens(text_df, words, text)
# remove stop-words using anti_join function from dplyr
# stop_words come from tidytext package
token_data <- anti_join(token_data, stop_words)
# use the count() function of dplyr to view most common words
wordcount <- count(token_data,word, sort = TRUE)
# tokenize with standard tokenization using unnest_tokens from tidytext
token_data <- unnest_tokens(text_df, words, text)
View(token_data)
# remove stop-words using anti_join function from dplyr
# stop_words come from tidytext package
token_data <- anti_join(token_data, stop_words)
View(token_data)
token_data <- unnest_tokens(text_df, by=c("text"="word"))
token_data <- unnest_tokens(text_df, words, text)
token_data <- anti_join(token_data, stop_words)
token_data <- anti_join(token_data, by=c("text"="word"))
token_data <- anti_join(token_data, stopwords)
token_data <- anti_join(token_data, "stopwords")
token_data <- anti_join(token_data, stop_words)
token_data <- anti_join(token_data, by="stop_words")
token_data %>% anti_join(stop_words)
# remove stop-words using anti_join function from dplyr
# stop_words come from tidytext package
token_data <- anti_join(token_data, stopwords)
token_data<- anti_join(stop_words)
library(tidytext)
library(dplyr)
library(ggplot2)
library(tidytext)
library(dplyr)
library(ggplot2)
claimsData <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv")
ncount <- nrow(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, words, text)
token_data <- unnest_tokens(text_df, words, text)
View(text_df)
token_data <- unnest_tokens(text_df, words, text)
text_df <- data_frame(line = 1:ncount, text = incidentData)
View(text_df)
head(text_df)
token_data <- unnest_tokens(text_df, words, text)
text_df <- data.frame(line = 1:ncount, text = incidentData)
head(text_df)
# tokenize with standard tokenization using unnest_tokens from tidytext
token_data <- unnest_tokens(text_df, words, text)
library("tidytext", lib.loc="~/R/win-library/3.5")
install.packages("dplyr")
install.packages("ggplot2")
library(tidytext)
library(dplyr)
library(ggplot2)
# install folowing packages and load the libraries
#     tidytext
#     dplyr
#     ggplot2
library(tidytext)
library(dplyr)
library(ggplot2)
ClaimsInjuryDescription <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv", stringsAsFactors=FALSE)
View(ClaimsInjuryDescription)
claimsData <- ClaimsInjuryDescription
ncount <- nrow(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data.frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, words, text)
token_data <- unnest_tokens(text_df, words, text)
# remove stop-words using anti_join function from dplyr
# stop_words come from tidytext package
token_data <- anti_join(token_data, stop_words)
# use the count() function of dplyr to view most common words
wordcount <- count(token_data,word, sort = TRUE)
# filter for n>5000 using filter function from dplyr
wordcountfiltered <- filter(wordcount, n > 2000)
# visualize with ggplot
ggplot(wordcountfiltered, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
ClaimsInjuryDescription <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv", stringsAsFactors=FALSE)
View(ClaimsInjuryDescription)
# store the data in a shorter named dataframe
claimsData <- ClaimsInjuryDescription
library(tidytext)
library(dplyr)
library(ggplot2)
ncount <- nrow(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data.frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, words, text)
JEOPARDY_CSV <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/JEOPARDY_CSV.csv", stringsAsFactors=FALSE)
View(JEOPARDY_CSV)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
Jeopardy <- JEOPARDY_CSV
JQCorpus <- Corpus(VectorSource(Jeopardy$Question))
JQCorpus <- tm_map(JQCorpus, content_transformer(tolower))
JQCorpus <- tm_map(JQCorpus, removePunctuation)
JQCorpus <- tm_map(JQCorpus, PlainTextDocument)
JQCorpus <- tm_map(JQCorpus, removeWords, stopwords('english'))
JQCorpus <- tm_map(JQCorpus, stemDocument)
JQCorpus <- Corpus(VectorSource(JQCorpus))
wordcloud(JQCorpus, max.words = 100, random.order = FALSE, colors = brewer.pal(6, "Dark2"))
ClaimsInjuryDescription <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Week 13/ClaimsInjuryDescription.csv", stringsAsFactors=FALSE)
View(ClaimsInjuryDescription)
library("tidytext", lib.loc="~/R/win-library/3.5")
library("dplyr", lib.loc="~/R/win-library/3.5")
library("ggplot2", lib.loc="~/R/win-library/3.5")
claimsData <- ClaimsInjuryDescription
ncount <- nrow(claimsData)
ncount
incidentData <- claimsData$IncidentDescription
text_df <- data_frame(line = 1:ncount, text = incidentData)
head(text_df)
token_data <- unnest_tokens(text_df, word, text)
token_data <- anti_join(token_data, stop_words)
wordcount <- count(token_data,word, sort = TRUE)
wordcountfiltered <- filter(wordcount, n > 2000)
ggplot(wordcountfiltered, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
JEOPARDY_Data <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Fall-2018/BD_6100/Extra Credit/JEOPARDY_Data.csv", stringsAsFactors=FALSE)
View(JEOPARDY_Data)
JData <- JEOPARDY_Data
ncount <- nrow(JData)
ncount
QuestionData <- JData$Question
text_df <- data_frame(line = 1:ncount, text = QuestionData)
head(text_df)
token_data <- unnest_tokens(text_df, word, text)
token_data <- anti_join(token_data, stop_words)
wordcount <- count(token_data,word, sort = TRUE)
wordcountfiltered <- filter(wordcount, n > 5000)
ggplot(wordcountfiltered, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
JData <- JEOPARDY_Data
ncount <- nrow(JData)
ncount
AnswerData <- JData$Answer
text_df <- data_frame(line = 1:ncount, text = AnswerData)
head(text_df)
token_data <- unnest_tokens(text_df, word, text)
token_data <- anti_join(token_data, stop_words)
wordcount <- count(token_data,word, sort = TRUE)
wordcountfiltered <- filter(wordcount, n > 5000)
# visualize with ggplot
ggplot(wordcountfiltered, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
wordcountfiltered <- filter(wordcount, n > 500)
# visualize with ggplot
ggplot(wordcountfiltered, aes(reorder(word, n), n)) +
geom_bar(stat = "identity") +
xlab(NULL) +
coord_flip()
View(JEOPARDY_Data)
View(JEOPARDY_Data)
install.packages("caret")
library(ellipse)
featurePlot(x=x, y=y, plot="ellipse")
library(caret)
knit_with_parameters('C:/Users/kanum/Desktop/Akshata/DSBA/Summer2019/Introduction to RStudio.Rmd')
unlink('C:/Users/kanum/Desktop/Akshata/DSBA/Summer2019/Introduction to RStudio_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
plot(cars)
knitr::opts_chunk$set(echo = TRUE)
# Chunk 1
plot(cars)
x = 1+1
y = 2*x
z <- (x+y)
1+2
salary <- read.csv(salary_data.csv)
salary <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Summer2019/Day7,8,9/salary_data.csv"")
salary <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Summer2019/Day7,8,9/salary_data.csv")
salary <- read.csv("C:/Users/kanum/Desktop/Akshata/DSBA/Summer2019/Day7,8,9/salary_data.csv")
# View the first 6 rows of data
head(salary)
# View the last 6 rows of data
tail(salary)
# View a condensed summary of the data
str(salary)
bmi <- readRDS('data/bmi.rds')
getwd()
setwd("C:/Users/kanum/Desktop/Akshata/mini_projects/Mobile_Price_Classification")
mydata <- read.table("train.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?"))
# Explore data
nrow(mydata)
#Summary of data
summary(mydata)
## Note: Since categorical variables enter into statistical models differently than continuous variables, storing data as factors insures that the modeling functions will treat such data correctly:
mydata$price_range <- as.factor(mydata$price_range)
# Install packages required for random forest:
install.packages("randomForest")
# Load packages required for random forest:
library(randomForest)
install.packages("randomForest")
# Load packages required for random forest:
library(randomForest)
rf <-randomForest(price_range~., data=train, ntree=10, na.action=na.exclude, importance=T,
proximity=T)
rf <-randomForest(price_range~., data=mydata, ntree=10, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=20, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=30, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=40, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=50, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=60, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=50, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=60, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=70, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#Buliding the rf model
#Change the number of ntree and repeat thia process till we reduce OOB estimate
rf <-randomForest(price_range~., data=mydata, ntree=60, na.action=na.exclude, importance=T,
proximity=T)
print(rf)
#To get optimal split of variables fix ntree=60
mtry <- tuneRF(mydata[-21], train_data$salary.class, ntreeTry=60,
stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE)
#To get optimal split of variables fix ntree=60
mtry <- tuneRF(mydata[-21], mydata$price_range, ntreeTry=60,
stepFactor=1.5, improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
rf <-randomForest(price_range~., data=mydata, mtry=best.m, importance=TRUE, ntree=60)
print(rf)
#Variable Importance which does Gini Importance in backend
importance(rf)
varImpPlot(rf)
# Calculate predictive probabilities of training dataset.
pred1 = predict(rf,type = "prob")
#Import my test data
testdata <- read.table("test.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?"))
test = df = subset(testdata, select = -c(price_range) )
View(testdata)
#Import my test data
testdata <- read.table("test.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?"))
View(mydata)
View(testdata)
View(mydata)
View(testdata)
predicted_values = predict(rf, type = "prob", testdata) # Use the rf classifier to make the predictions
final_data <- cbind(testdata, predicted_values) # Add the predictions to test_data
View(final_data)
